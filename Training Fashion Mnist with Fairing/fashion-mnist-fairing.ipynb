{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fashion mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200810 14:55:15 config:134] Using preprocessor: <kubeflow.fairing.preprocessors.converted_notebook.ConvertNotebookPreprocessor object at 0x7f1ba142dd30>\n",
      "[I 200810 14:55:15 config:136] Using builder: <kubeflow.fairing.builders.append.append.AppendBuilder object at 0x7f1b52f3c518>\n",
      "[I 200810 14:55:15 config:138] Using deployer: <kubeflow.fairing.deployers.job.job.Job object at 0x7f1b53cc2f28>\n",
      "[W 200810 14:55:15 append:50] Building image using Append builder...\n",
      "[I 200810 14:55:15 base:107] Creating docker context: /tmp/fairing_context_zwxus7wj\n",
      "[I 200810 14:55:16 converted_notebook:127] Converting fashion-mnist-fairing.ipynb to fashion-mnist-fairing.py\n",
      "[I 200810 14:55:16 docker_creds_:234] Loading Docker credentials for repository 'jaewoo201/kubeflow-jupyter-lab:tf2.0-cpu'\n",
      "[W 200810 14:55:17 append:54] Image successfully built in 2.006140182958916s.\n",
      "[W 200810 14:55:17 append:94] Pushing image jaewoo201/fairing-job:6275EA61...\n",
      "[I 200810 14:55:17 docker_creds_:234] Loading Docker credentials for repository 'jaewoo201/fairing-job:6275EA61'\n",
      "[W 200810 14:55:17 append:81] Uploading jaewoo201/fairing-job:6275EA61\n",
      "[I 200810 14:55:19 docker_session_:284] Layer sha256:b1eef8809d53d9611d6f6f27ccf3406ed789d82867b02f2ad854bb3ed016af86 pushed.\n",
      "[I 200810 14:55:19 docker_session_:284] Layer sha256:1ad1987e69f42fc1f0886cf57f4d31fa5c100cfbe37d341d131b4ba4f56beb09 pushed.\n",
      "[I 200810 14:55:20 docker_session_:284] Layer sha256:d83811f270d56d34a208f721f3dbf1b9242d1900ad8981fc7071339681998a31 pushed.\n",
      "[I 200810 14:55:20 docker_session_:284] Layer sha256:7fc152dfb3a6b5c9a436b49ff6cd72ed7eb5f1fd349128b50ee04c3c5c2355fb pushed.\n",
      "[I 200810 14:55:20 docker_session_:284] Layer sha256:af228bb15b867acbee0d6df28dbb42a8218948fdde64833bd021a255a99a2690 pushed.\n",
      "[I 200810 14:55:20 docker_session_:284] Layer sha256:95e1acc3257281f4999555421c924f2c484f7466a45f699731edeba6dc401a6a pushed.\n",
      "[I 200810 14:55:21 docker_session_:284] Layer sha256:66257906239d377cd700c566b27f12895ccefcd8d95eae7377f600208151d8e0 pushed.\n",
      "[I 200810 14:55:21 docker_session_:284] Layer sha256:4c9126e356caed331bb06f0fc812e8573efd887d8706ae4c0e0876c59757a5e5 pushed.\n",
      "[I 200810 14:55:21 docker_session_:284] Layer sha256:0d76df7e77103be055d8ce69a577033eb9f7451221669687b25092308b9c51b6 pushed.\n",
      "[I 200810 14:55:23 docker_session_:284] Layer sha256:7c079a67ba7abde01dd87da063d5b33172c2b90318fb1a38240ed3926695a1f5 pushed.\n",
      "[I 200810 14:55:23 docker_session_:284] Layer sha256:5667fdb72017d1fb364744ca1abf7b6f3bbe9c98c3786f294a461c2866db69ab pushed.\n",
      "[I 200810 14:55:23 docker_session_:284] Layer sha256:1dbff9c5a25f09e41d70b8c76beec67b47cfa8b1f10036f1829371ce6f1e226f pushed.\n",
      "[I 200810 14:55:23 docker_session_:284] Layer sha256:1c60aaf19cc4c142934f390281975f61956475c205b5895711f839a91f927779 pushed.\n",
      "[I 200810 14:55:24 docker_session_:284] Layer sha256:533437d4de23a65cb793d50c82a05cf77952f8a14ec7e593498c135a10e617af pushed.\n",
      "[I 200810 14:55:24 docker_session_:284] Layer sha256:ee671aafb583e2321880e275c94d49a49185006730e871435cd851f42d2a775d pushed.\n",
      "[I 200810 14:55:25 docker_session_:284] Layer sha256:fb9676c4102241f220b864e98c53676e27c407c6ecfbb13c2cbc1fa4de8f1811 pushed.\n",
      "[I 200810 14:55:26 docker_session_:284] Layer sha256:259e9375f66b57ed03f5060df6d8c28b806696c0914e3b7ca1b544bf19477014 pushed.\n",
      "[I 200810 14:55:27 docker_session_:284] Layer sha256:0154a40173948b1d5827a478e60b318d2c71fba1cbb49a88eccab09e9ef27536 pushed.\n",
      "[I 200810 14:55:28 docker_session_:284] Layer sha256:8e7d3ffcd500d3ed0b2ee2449a36a44601b8e282cc6d541ab93f1189d4a2cf93 pushed.\n",
      "[I 200810 14:55:30 docker_session_:284] Layer sha256:e5297a7e170d7c5c99d8143acc56c628999026b97fd98081a44c1565d0a2eddd pushed.\n",
      "[I 200810 14:55:33 docker_session_:284] Layer sha256:7ce18cac5d420d4c9d5862ece86da73a3afb91f178867f7b4452a7135a233425 pushed.\n",
      "[I 200810 14:55:37 docker_session_:284] Layer sha256:485ea80d093de507a5ccd36effaf8dad03b67a1063edbd427fedfe667c9155bc pushed.\n",
      "[I 200810 14:55:52 docker_session_:284] Layer sha256:c2f81144f815902f9c6e6f7883068544e35924f0efe2275bc41c3d0c558f394c pushed.\n",
      "[I 200810 14:56:26 docker_session_:284] Layer sha256:ee2ff60054df97fbcfb640c0aa9cbd5d6658c00a9b5d44f5eb6f9ff2f269158f pushed.\n",
      "[I 200810 14:56:27 docker_session_:334] Finished upload of: jaewoo201/fairing-job:6275EA61\n",
      "[W 200810 14:56:27 append:99] Pushed image jaewoo201/fairing-job:6275EA61 in 69.60805224708747s.\n",
      "[W 200810 14:56:27 job:101] The job fairing-job-g4x22 launched.\n",
      "[W 200810 14:56:28 manager:298] Waiting for fairing-job-g4x22-6r4s8 to start...\n",
      "[W 200810 14:56:28 manager:298] Waiting for fairing-job-g4x22-6r4s8 to start...\n",
      "[W 200810 14:56:28 manager:298] Waiting for fairing-job-g4x22-6r4s8 to start...\n",
      "[I 200810 14:57:55 manager:304] Pod started running True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-10 14:57:56.274180: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2020-08-10 14:57:56.274228: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "2020-08-10 14:58:00.329010: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2020-08-10 14:58:00.329049: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2020-08-10 14:58:00.329080: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (fairing-job-g4x22-6r4s8): /proc/driver/nvidia/version does not exist\n",
      "2020-08-10 14:58:00.329425: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2020-08-10 14:58:00.340038: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2294685000 Hz\n",
      "2020-08-10 14:58:00.340985: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x63e3a40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-08-10 14:58:00.341019: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #\n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0\n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               100480\n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0\n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290\n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      " 124/1875 [>.............................] - ETA: 6s - loss: 1.0144 - accuracy: 0.64\n",
      " 290/1875 [===>..........................] - ETA: 5s - loss: 0.8064 - accuracy: 0.72\n",
      " 398/1875 [=====>........................] - ETA: 5s - loss: 0.7518 - accuracy: 0.74\n",
      " 504/1875 [=======>......................] - ETA: 5s - loss: 0.7100 - accuracy: 0.75\n",
      " 663/1875 [=========>....................] - ETA: 4s - loss: 0.6674 - accuracy: 0.76\n",
      " 798/1875 [===========>..................] - ETA: 3s - loss: 0.6407 - accuracy: 0.77\n",
      " 956/1875 [==============>...............] - ETA: 3s - loss: 0.6152 - accuracy: 0.78\n",
      "1039/1875 [===============>..............] - ETA: 3s - loss: 0.6050 - accuracy: 0.78\n",
      "1172/1875 [=================>............] - ETA: 2s - loss: 0.5920 - accuracy: 0.79\n",
      "1333/1875 [====================>.........] - ETA: 2s - loss: 0.5767 - accuracy: 0.79\n",
      "1495/1875 [======================>.......] - ETA: 1s - loss: 0.5608 - accuracy: 0.80\n",
      "1608/1875 [========================>.....] - ETA: 1s - loss: 0.5520 - accuracy: 0.80\n",
      "1745/1875 [==========================>...] - ETA: 0s - loss: 0.5437 - accuracy: 0.807\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.5356 - accuracy: 0.8094\n",
      "Epoch 2/10\n",
      "  82/1875 [>.............................] - ETA: 5s - loss: 0.4204 - accuracy: 0.84\n",
      " 187/1875 [=>............................] - ETA: 6s - loss: 0.4030 - accuracy: 0.84\n",
      " 287/1875 [===>..........................] - ETA: 6s - loss: 0.4079 - accuracy: 0.84\n",
      " 450/1875 [======>.......................] - ETA: 5s - loss: 0.4052 - accuracy: 0.85\n",
      " 556/1875 [=======>......................] - ETA: 4s - loss: 0.4092 - accuracy: 0.85\n",
      " 692/1875 [==========>...................] - ETA: 4s - loss: 0.4041 - accuracy: 0.85\n",
      " 827/1875 [============>.................] - ETA: 3s - loss: 0.4089 - accuracy: 0.85\n",
      " 989/1875 [==============>...............] - ETA: 3s - loss: 0.4076 - accuracy: 0.85\n",
      "1151/1875 [=================>............] - ETA: 2s - loss: 0.4044 - accuracy: 0.85\n",
      "1285/1875 [===================>..........] - ETA: 2s - loss: 0.4039 - accuracy: 0.85\n",
      "1445/1875 [=====================>........] - ETA: 1s - loss: 0.4024 - accuracy: 0.85\n",
      "1545/1875 [=======================>......] - ETA: 1s - loss: 0.4014 - accuracy: 0.85\n",
      "1656/1875 [=========================>....] - ETA: 0s - loss: 0.4017 - accuracy: 0.854\n",
      "1792/1875 [===========================>..] - ETA: 0s - loss: 0.4002 - accuracy: 0.855\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3984 - accuracy: 0.8554\n",
      "Epoch 3/10\n",
      " 108/1875 [>.............................] - ETA: 6s - loss: 0.3817 - accuracy: 0.86\n",
      " 271/1875 [===>..........................] - ETA: 5s - loss: 0.3677 - accuracy: 0.86\n",
      " 403/1875 [=====>........................] - ETA: 5s - loss: 0.3713 - accuracy: 0.86\n",
      " 541/1875 [=======>......................] - ETA: 4s - loss: 0.3652 - accuracy: 0.86\n",
      " 672/1875 [=========>....................] - ETA: 4s - loss: 0.3641 - accuracy: 0.86\n",
      " 833/1875 [============>.................] - ETA: 3s - loss: 0.3693 - accuracy: 0.86\n",
      " 968/1875 [==============>...............] - ETA: 3s - loss: 0.3675 - accuracy: 0.86\n",
      "1098/1875 [================>.............] - ETA: 2s - loss: 0.3676 - accuracy: 0.86\n",
      "1183/1875 [=================>............] - ETA: 2s - loss: 0.3671 - accuracy: 0.86\n",
      "1318/1875 [====================>.........] - ETA: 2s - loss: 0.3681 - accuracy: 0.86\n",
      "1455/1875 [=====================>........] - ETA: 1s - loss: 0.3672 - accuracy: 0.86\n",
      "1591/1875 [========================>.....] - ETA: 1s - loss: 0.3669 - accuracy: 0.86\n",
      "1723/1875 [==========================>...] - ETA: 0s - loss: 0.3665 - accuracy: 0.866\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3655 - accuracy: 0.8661\n",
      "Epoch 4/10\n",
      " 115/1875 [>.............................] - ETA: 7s - loss: 0.3499 - accuracy: 0.8725\n",
      " 246/1875 [==>...........................] - ETA: 6s - loss: 0.3501 - accuracy: 0.869\n",
      " 383/1875 [=====>........................] - ETA: 5s - loss: 0.3458 - accuracy: 0.870\n",
      " 519/1875 [=======>......................] - ETA: 5s - loss: 0.3457 - accuracy: 0.871\n",
      " 653/1875 [=========>....................] - ETA: 4s - loss: 0.3438 - accuracy: 0.872\n",
      " 759/1875 [===========>..................] - ETA: 4s - loss: 0.3432 - accuracy: 0.873\n",
      " 867/1875 [============>.................] - ETA: 3s - loss: 0.3409 - accuracy: 0.87\n",
      "1029/1875 [===============>..............] - ETA: 3s - loss: 0.3427 - accuracy: 0.87\n",
      "1189/1875 [==================>...........] - ETA: 2s - loss: 0.3445 - accuracy: 0.87\n",
      "1304/1875 [===================>..........] - ETA: 2s - loss: 0.3447 - accuracy: 0.87\n",
      "1466/1875 [======================>.......] - ETA: 1s - loss: 0.3434 - accuracy: 0.87\n",
      "1599/1875 [========================>.....] - ETA: 1s - loss: 0.3432 - accuracy: 0.87\n",
      "1764/1875 [===========================>..] - ETA: 0s - loss: 0.3432 - accuracy: 0.87\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3430 - accuracy: 0.8734\n",
      "Epoch 5/10\n",
      " 130/1875 [=>............................] - ETA: 6s - loss: 0.3254 - accuracy: 0.88\n",
      " 287/1875 [===>..........................] - ETA: 5s - loss: 0.3367 - accuracy: 0.87\n",
      " 371/1875 [====>.........................] - ETA: 5s - loss: 0.3278 - accuracy: 0.88\n",
      " 479/1875 [======>.......................] - ETA: 5s - loss: 0.3308 - accuracy: 0.87\n",
      " 640/1875 [=========>....................] - ETA: 4s - loss: 0.3342 - accuracy: 0.87\n",
      " 777/1875 [===========>..................] - ETA: 4s - loss: 0.3330 - accuracy: 0.87\n",
      " 937/1875 [=============>................] - ETA: 3s - loss: 0.3327 - accuracy: 0.87\n",
      "1070/1875 [================>.............] - ETA: 2s - loss: 0.3328 - accuracy: 0.87\n",
      "1208/1875 [==================>...........] - ETA: 2s - loss: 0.3333 - accuracy: 0.87\n",
      "1342/1875 [====================>.........] - ETA: 2s - loss: 0.3318 - accuracy: 0.87\n",
      "1502/1875 [======================>.......] - ETA: 1s - loss: 0.3300 - accuracy: 0.87\n",
      "1613/1875 [========================>.....] - ETA: 1s - loss: 0.3293 - accuracy: 0.87\n",
      "1718/1875 [==========================>...] - ETA: 0s - loss: 0.3297 - accuracy: 0.877\n",
      "1818/1875 [============================>.] - ETA: 0s - loss: 0.3282 - accuracy: 0.878\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3281 - accuracy: 0.8791\n",
      "Epoch 6/10\n",
      " 132/1875 [=>............................] - ETA: 6s - loss: 0.3200 - accuracy: 0.87\n",
      " 266/1875 [===>..........................] - ETA: 5s - loss: 0.3332 - accuracy: 0.87\n",
      " 425/1875 [=====>........................] - ETA: 5s - loss: 0.3231 - accuracy: 0.88\n",
      " 583/1875 [========>.....................] - ETA: 4s - loss: 0.3251 - accuracy: 0.87\n",
      " 720/1875 [==========>...................] - ETA: 4s - loss: 0.3232 - accuracy: 0.88\n",
      " 822/1875 [============>.................] - ETA: 3s - loss: 0.3192 - accuracy: 0.88\n",
      " 984/1875 [==============>...............] - ETA: 3s - loss: 0.3199 - accuracy: 0.88\n",
      "1117/1875 [================>.............] - ETA: 2s - loss: 0.3193 - accuracy: 0.88\n",
      "1249/1875 [==================>...........] - ETA: 2s - loss: 0.3176 - accuracy: 0.88\n",
      "1351/1875 [====================>.........] - ETA: 1s - loss: 0.3176 - accuracy: 0.88\n",
      "1428/1875 [=====================>........] - ETA: 1s - loss: 0.3171 - accuracy: 0.88\n",
      "1523/1875 [======================>.......] - ETA: 1s - loss: 0.3173 - accuracy: 0.88\n",
      "1632/1875 [=========================>....] - ETA: 0s - loss: 0.3167 - accuracy: 0.882\n",
      "1764/1875 [===========================>..] - ETA: 0s - loss: 0.3169 - accuracy: 0.882\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.3174 - accuracy: 0.882\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3174 - accuracy: 0.8825\n",
      "Epoch 7/10\n",
      " 136/1875 [=>............................] - ETA: 6s - loss: 0.2789 - accuracy: 0.89\n",
      " 296/1875 [===>..........................] - ETA: 5s - loss: 0.2913 - accuracy: 0.88\n",
      " 436/1875 [=====>........................] - ETA: 5s - loss: 0.2963 - accuracy: 0.88\n",
      " 573/1875 [========>.....................] - ETA: 4s - loss: 0.2995 - accuracy: 0.88\n",
      " 705/1875 [==========>...................] - ETA: 4s - loss: 0.3034 - accuracy: 0.88\n",
      " 863/1875 [============>.................] - ETA: 3s - loss: 0.3062 - accuracy: 0.88\n",
      " 993/1875 [==============>...............] - ETA: 3s - loss: 0.3087 - accuracy: 0.88\n",
      "1078/1875 [================>.............] - ETA: 2s - loss: 0.3082 - accuracy: 0.88\n",
      "1234/1875 [==================>...........] - ETA: 2s - loss: 0.3076 - accuracy: 0.88\n",
      "1370/1875 [====================>.........] - ETA: 1s - loss: 0.3077 - accuracy: 0.88\n",
      "1496/1875 [======================>.......] - ETA: 1s - loss: 0.3089 - accuracy: 0.88\n",
      "1641/1875 [========================>.....] - ETA: 0s - loss: 0.3100 - accuracy: 0.88\n",
      "1776/1875 [===========================>..] - ETA: 0s - loss: 0.3083 - accuracy: 0.885\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3096 - accuracy: 0.8848\n",
      "Epoch 8/10\n",
      "  92/1875 [>.............................] - ETA: 7s - loss: 0.2966 - accuracy: 0.8922\n",
      " 230/1875 [==>...........................] - ETA: 6s - loss: 0.2914 - accuracy: 0.891\n",
      " 393/1875 [=====>........................] - ETA: 5s - loss: 0.3019 - accuracy: 0.887\n",
      " 554/1875 [=======>......................] - ETA: 4s - loss: 0.3042 - accuracy: 0.886\n",
      " 657/1875 [=========>....................] - ETA: 4s - loss: 0.3005 - accuracy: 0.887\n",
      " 739/1875 [==========>...................] - ETA: 4s - loss: 0.2974 - accuracy: 0.888\n",
      " 902/1875 [=============>................] - ETA: 3s - loss: 0.2951 - accuracy: 0.88\n",
      "1037/1875 [===============>..............] - ETA: 3s - loss: 0.2948 - accuracy: 0.89\n",
      "1146/1875 [=================>............] - ETA: 2s - loss: 0.2961 - accuracy: 0.88\n",
      "1282/1875 [===================>..........] - ETA: 2s - loss: 0.2987 - accuracy: 0.88\n",
      "1421/1875 [=====================>........] - ETA: 1s - loss: 0.2985 - accuracy: 0.88\n",
      "1525/1875 [=======================>......] - ETA: 1s - loss: 0.2989 - accuracy: 0.88\n",
      "1660/1875 [=========================>....] - ETA: 0s - loss: 0.2981 - accuracy: 0.88\n",
      "1796/1875 [===========================>..] - ETA: 0s - loss: 0.2984 - accuracy: 0.88\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2986 - accuracy: 0.8888\n",
      "Epoch 9/10\n",
      " 133/1875 [=>............................] - ETA: 6s - loss: 0.2768 - accuracy: 0.89\n",
      " 244/1875 [==>...........................] - ETA: 5s - loss: 0.2819 - accuracy: 0.89\n",
      " 351/1875 [====>.........................] - ETA: 5s - loss: 0.2833 - accuracy: 0.89\n",
      " 458/1875 [======>.......................] - ETA: 5s - loss: 0.2803 - accuracy: 0.89\n",
      " 619/1875 [========>.....................] - ETA: 4s - loss: 0.2813 - accuracy: 0.89\n",
      " 782/1875 [===========>..................] - ETA: 4s - loss: 0.2835 - accuracy: 0.89\n",
      " 951/1875 [==============>...............] - ETA: 3s - loss: 0.2862 - accuracy: 0.89\n",
      "1084/1875 [================>.............] - ETA: 2s - loss: 0.2853 - accuracy: 0.89\n",
      "1220/1875 [==================>...........] - ETA: 2s - loss: 0.2869 - accuracy: 0.89\n",
      "1385/1875 [=====================.........] - ETA: 1s - loss: 0.2879 - accuracy: 0.89\n",
      "1521/1875 [======================>.......] - ETA: 1s - loss: 0.2879 - accuracy: 0.89\n",
      "1656/1875 [=========================>....] - ETA: 0s - loss: 0.2888 - accuracy: 0.89\n",
      "1758/1875 [===========================>..] - ETA: 0s - loss: 0.2893 - accuracy: 0.892\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2896 - accuracy: 0.8923\n",
      "Epoch 10/10\n",
      " 131/1875 [=>............................] - ETA: 6s - loss: 0.2764 - accuracy: 0.89\n",
      " 265/1875 [===>..........................] - ETA: 6s - loss: 0.2822 - accuracy: 0.89\n",
      " 400/1875 [=====>........................] - ETA: 5s - loss: 0.2836 - accuracy: 0.89\n",
      " 481/1875 [======>.......................] - ETA: 5s - loss: 0.2870 - accuracy: 0.89\n",
      " 641/1875 [=========>....................] - ETA: 4s - loss: 0.2871 - accuracy: 0.89\n",
      " 777/1875 [===========>..................] - ETA: 4s - loss: 0.2838 - accuracy: 0.89\n",
      " 943/1875 [==============>...............] - ETA: 3s - loss: 0.2830 - accuracy: 0.89\n",
      "1104/1875 [================>.............] - ETA: 2s - loss: 0.2821 - accuracy: 0.89\n",
      "1266/1875 [===================>..........] - ETA: 2s - loss: 0.2836 - accuracy: 0.89\n",
      "1368/1875 [====================>.........] - ETA: 1s - loss: 0.2844 - accuracy: 0.89\n",
      "1483/1875 [======================>.......] - ETA: 1s - loss: 0.2851 - accuracy: 0.89\n",
      "1632/1875 [========================>.....] - ETA: 0s - loss: 0.2846 - accuracy: 0.89\n",
      "1721/1875 [==========================>...] - ETA: 0s - loss: 0.2849 - accuracy: 0.893\n",
      "1858/1875 [============================>.] - ETA: 0s - loss: 0.2843 - accuracy: 0.892\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2842 - accuracy: 0.8929\n",
      "313/313 - 1s - loss: 0.3373 - accuracy: 0.8837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 200810 14:59:12 job:173] Cleaning up job fairing-job-g4x22...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "class MyFashionMnist(object):\n",
    "    def train(self):\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "        x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "        model = tf.keras.models.Sequential([\n",
    "          tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "          tf.keras.layers.Dense(128, activation='relu'),\n",
    "          tf.keras.layers.Dropout(0.2),\n",
    "          tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "        model.summary()\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        model.fit(x_train, y_train, epochs=10)\n",
    "\n",
    "        model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if os.getenv('FAIRING_RUNTIME', None) is None:\n",
    "        from kubeflow import fairing\n",
    "        from kubeflow.fairing.kubernetes import utils as k8s_utils\n",
    "\n",
    "        DOCKER_REGISTRY = 'jaewoo201'\n",
    "        fairing.config.set_builder(\n",
    "            'append',\n",
    "            base_image='jaewoo201/kubeflow-jupyter-lab:tf2.0-cpu',\n",
    "            registry=DOCKER_REGISTRY, \n",
    "            push=True)\n",
    "        # cpu 2, memory 5GiB\n",
    "        fairing.config.set_deployer('job',\n",
    "                                    pod_spec_mutators=[\n",
    "                                        k8s_utils.get_resource_mutator(cpu=1,\n",
    "                                                                       memory=2)]\n",
    "         \n",
    "                                   )\n",
    "        fairing.config.run()\n",
    "    else:\n",
    "        remote_train = MyFashionMnist()\n",
    "        remote_train.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
